# ğŸ›’ Retail Analytics Platform

**A personal end-to-end data platform project** designed to simulate the architecture, tooling, and workflows of a modern retail analytics stack. This project demonstrates best practices across the full data lifecycle: ingestion, orchestration, transformation, API exposure, and deployment.

### Overview

This platform processes simulated point-of-sale (POS) and inventory data through a complete pipeline:

- Ingests batch data into **Google Cloud Storage** and loads into **BigQuery**
- Applies data quality checks and transformations using **dbt**
- Orchestrates workflows with **Airflow**
- Exposes endpoints via a **FastAPI** microservice
- Deploys to **GKE** with full CI/CD, observability, and infrastructure as code via **Terraform** using **Githun Actions**.

ğŸ“Œ **Stack**: Python Â· BigQuery Â· dbt Â· Airflow Â· FastAPI Â· GCP Â· Kubernetes Â· Terraform Â· GitHub Actions Â· Redis Â· Docker Â· Kafka Â· Grafana

---

## Project Structure

| Folder | Description |
|--------|-------------|
| [`data-ingestion/`](./data-ingestion/) | Python scripts or DAGs for ingesting simulated retail data into GCS and BigQuery |
| [`dbt/`](./dbt/) | dbt project containing models, seeds, tests, macros and documentation |
| [`airflow/`](./airflow/) | Airflow DAGs and configuration, including Cosmos integration |
| [`api/`](./api/) | FastAPI service exposing endpoints for product, customer, and sales analytics |
| [`terraform/`](./terraform/) | Infrastructure as code for provisioning GCP resources and GKE clusters |
| [`monitoring/`](./monitoring/) | Observability setup using Prometheus, Grafana, and GCP Cloud Operations |
| [`docs/`](./docs/) | Rendered documentation (e.g. dbt docs, architectural diagrams, OpenAPI schemas) |
| [`scripts/`](./scripts/) | Utility scripts for testing, local development, and data simulation |

---

## ğŸ” Project Goals

This repository supports a structured **4-month personal development plan** focused on:

1. **Data Engineering Foundations** â€“ modelling, warehousing, Airflow, BigQuery, CDC
2. **Backend Engineering** â€“ API design, auth, testing, caching, async processing
3. **Software Engineering** â€“ design patterns, system design, Kafka integration, TDD
4. **DevOps & Cloud** â€“ CI/CD, Terraform, Kubernetes, observability

---

## Component-Level Documentation

Each module contains its own `README.md` with implementation-specific details and usage guides:

- ğŸ“¦ [`dbt/`](./dbt/) â€“ dbt project setup with model definitions.
- â±ï¸ [`airflow/`](./airflow/) â€“ DAG definitions, Cosmos setup
- âš™ï¸ [`api/`](./api/) â€“ FastAPI endpoints, OpenAPI, authentication
- â˜ï¸ [`terraform/`](./terraform/) â€“ GCP provisioning, GKE setup
- ğŸ“ˆ [`monitoring/`](./monitoring/) â€“ Metrics scraping and dashboards

---

## Additional Resources

- **[dbt Documentation](https://docs.getdbt.com/)** â€“ Modelling and transformation
- **[Astronomer Cosmos](https://astronomer.github.io/astronomer-cosmos/)** â€“ Airflow-dbt integration
- **[BigQuery Docs](https://cloud.google.com/bigquery/docs/)** â€“ Serverless warehousing
- **[FastAPI Docs](https://fastapi.tiangolo.com/)** â€“ Modern Python web APIs
- **[Terraform GCP Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs)** â€“ Infrastructure provisioning

---

## Status

ğŸ› ï¸ **In Progress** â€“ Iteratively built as part of a continuous learning journey. Major components are implemented in phases and tracked via internal planning.

---
